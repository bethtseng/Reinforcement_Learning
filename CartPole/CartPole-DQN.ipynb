{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CartPole-v0\n",
    "\n",
    "> reference: https://keon.io/deep-q-learning/\n",
    "\n",
    "A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. \n",
    "\n",
    "A `reward` of +1 is provided for every timestep that the pole remains upright. The episode ends when `the pole is more than 15 degrees from vertical`, or `the cart moves more than 2.4 units from the center`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.callbacks import History\n",
    "from collections import deque\n",
    "import itertools\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent():\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self._state_size = state_size\n",
    "        self._action_size = action_size        \n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self._learning_rate = 0.001\n",
    "        self.model_name = 'CartPole-DQN.h5'\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "    \n",
    "    \n",
    "    def _build_model(self, hidden_size=24):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(hidden_size, activation='relu', kernel_initializer='glorot_uniform', input_shape=(self._state_size,)))\n",
    "        model.add(Dense(hidden_size, activation='relu', kernel_initializer='glorot_uniform', input_dim=hidden_size))\n",
    "        model.add(Dense(self._action_size, activation='linear', kernel_initializer='glorot_uniform', input_dim=hidden_size))\n",
    "        opt = Adam(lr=self._learning_rate)\n",
    "        model.compile(loss='mean_squared_error', optimizer=opt)\n",
    "        model.summary()\n",
    "        if os.path.exists(self.model_name):\n",
    "            model.load_weights(self.model_name)\n",
    "        return model\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def update_target_model(self):\n",
    "        self.target_model = self.model\n",
    "        \n",
    "    def train_model(self, batch_size=20):\n",
    "        def batch(iterable, n=1):\n",
    "            l = len(iterable)\n",
    "            for ndx in range(0, l, n):\n",
    "                yield list(itertools.islice(iterable, ndx, min(ndx + n, l)))\n",
    "        \n",
    "        history = History()\n",
    "        batch_data = list(batch(self.memory, batch_size))\n",
    "        selected_batch = random.sample(batch_data, 1)\n",
    "        \n",
    "        for state, action, reward, next_state, done in selected_batch[0]:\n",
    "            target = self.model.predict(state)\n",
    "            target_val = self.model.predict(next_state)\n",
    "            target_val_ = self.target_model.predict(next_state)\n",
    "            if done:\n",
    "                target[0][action] = reward\n",
    "            else:\n",
    "                a = np.argmax(target_val)\n",
    "                target[0][action] = reward + self.gamma * target_val_[0][a]\n",
    "            \n",
    "            self.model.fit(state, target, epochs=1, verbose=0, callbacks=[history])\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay    \n",
    "\n",
    "    def takeAction(self, state):\n",
    "        state = state.reshape(1,self._state_size)\n",
    "        act = np.argmax(self.model.predict(state))\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            act = random.randrange(self._action_size)\n",
    "        return act\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 24)                120       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 50        \n",
      "=================================================================\n",
      "Total params: 770\n",
      "Trainable params: 770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 24)                120       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 2)                 50        \n",
      "=================================================================\n",
      "Total params: 770\n",
      "Trainable params: 770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Episode: 0, Total Reward: 11.0\n",
      "Episode: 20, Total Reward: 22.15\n",
      "Episode: 40, Total Reward: 21.6\n",
      "Episode: 60, Total Reward: 18.05\n",
      "Episode: 80, Total Reward: 25.0\n",
      "Episode: 100, Total Reward: 21.85\n",
      "Episode: 120, Total Reward: 29.45\n",
      "Episode: 140, Total Reward: 38.4\n",
      "Episode: 160, Total Reward: 34.5\n",
      "Episode: 180, Total Reward: 40.05\n",
      "Episode: 200, Total Reward: 56.1\n",
      "Episode: 220, Total Reward: 41.25\n",
      "Episode: 240, Total Reward: 58.25\n",
      "Episode: 260, Total Reward: 91.45\n",
      "Episode: 280, Total Reward: 135.55\n",
      "Episode: 300, Total Reward: 117.05\n",
      "Episode: 320, Total Reward: 162.25\n",
      "Episode: 340, Total Reward: 144.55\n",
      "Episode: 360, Total Reward: 154.9\n",
      "Episode: 380, Total Reward: 185.6\n",
      "Episode: 400, Total Reward: 150.65\n",
      "Episode: 420, Total Reward: 119.15\n",
      "Episode: 440, Total Reward: 93.5\n",
      "Episode: 460, Total Reward: 77.35\n",
      "Episode: 480, Total Reward: 111.75\n"
     ]
    }
   ],
   "source": [
    "episodes = 500\n",
    "render = False\n",
    "shows = 20\n",
    "total_reward = list()\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "state = env.reset()\n",
    "agent = DQNAgent(state.shape[0], env.action_space.n)\n",
    "\n",
    "for e in range(episodes):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, 4])\n",
    "    \n",
    "    for time_t in range(500):\n",
    "        \n",
    "        if render:\n",
    "            env.render()\n",
    "            \n",
    "        action = agent.takeAction(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        next_state = np.reshape(next_state, [1, 4])\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            total_reward.append(time_t)\n",
    "            break\n",
    "            \n",
    "    agent.train_model(32)\n",
    "    \n",
    "    if e % shows == 0:\n",
    "        print(\"Episode: {}, Total Reward: {}\".format(e, np.mean(total_reward[-shows:])))\n",
    "        agent.update_target_model()\n",
    "        agent.model.save_weights(agent.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py36(openAI gym)",
   "language": "python",
   "name": "openai-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
