{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CartPole-v0\n",
    "\n",
    "> reference: \n",
    "\n",
    "A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. \n",
    "\n",
    "A `reward` of +1 is provided for every timestep that the pole remains upright. The episode ends when `the pole is more than 15 degrees from vertical`, or `the cart moves more than 2.4 units from the center`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym, threading, random, itertools, time\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Brain():\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self._state_size = state_size\n",
    "        self._action_size = action_size     \n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self._learning_rate = 0.001\n",
    "        \n",
    "        self.value_size = 1        \n",
    "        self.actor = self._build_actor()\n",
    "        self.critic = self._build_critic()\n",
    "        \n",
    "        # tensorflow\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def _build_actor(my, hidden_size=24):\n",
    "        class actorModel():\n",
    "            def __init__(self):\n",
    "                self.x = tf.placeholder(tf.float32, [None, my._state_size])\n",
    "                nn = tf.layers.dense(self.x, hidden_size, activation=tf.nn.relu)\n",
    "                nn = tf.layers.dense(nn, hidden_size, activation=tf.nn.relu)\n",
    "                all_act = tf.layers.dense(nn, my._action_size)#, activation=tf.nn.linear)\n",
    "                self.all_act_prob = tf.nn.softmax(all_act, name='act_prob')\n",
    "        \n",
    "                # op to sample an action\n",
    "                self._sample = tf.reshape(tf.multinomial(self.all_act_prob, 1), [])\n",
    "\n",
    "                self.advantages = tf.placeholder(tf.float32, [None, my._action_size]) \n",
    "                self.loss = -tf.reduce_sum(tf.multiply(self.all_act_prob, self.advantages))\n",
    "\n",
    "                # update\n",
    "                optimizer = tf.train.AdamOptimizer(my._learning_rate)\n",
    "                self.train_op = optimizer.minimize(self.loss)\n",
    "                \n",
    "                # Summary for TensorBoard\n",
    "                tf.summary.scalar('loss', self.loss)\n",
    "                self.summary = tf.summary.merge_all()    # summary\n",
    "                \n",
    "        return actorModel()\n",
    "    \n",
    "    # critic: state is input and value of state is output of model\n",
    "    def _build_critic(my, hidden_size=24):\n",
    "        class criticModel():\n",
    "            def __init__(self):\n",
    "                self.x = tf.placeholder(tf.float32, [None, my._state_size])\n",
    "                nn = tf.layers.dense(self.x, hidden_size, activation=tf.nn.relu)\n",
    "                nn = tf.layers.dense(nn, hidden_size, activation=tf.nn.relu)\n",
    "                self.Q = tf.layers.dense(nn, my.value_size)#, activation=tf.nn.linear)\n",
    "        \n",
    "                # Loss\n",
    "                self.targetQ = tf.placeholder(tf.float32, [None, my.value_size])\n",
    "                self.loss = tf.reduce_mean((self.Q - self.targetQ)**2)\n",
    "                \n",
    "                # Summary for TensorBoard\n",
    "                tf.summary.scalar('loss', self.loss)\n",
    "                self.summary = tf.summary.merge_all()    # summary\n",
    "                \n",
    "                # Training\n",
    "                self.train_op = tf.train.AdamOptimizer(my._learning_rate).minimize(self.loss)   \n",
    "                \n",
    "        return criticModel()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2CAgent():\n",
    "    def __init__(self):      \n",
    "        self.memory = deque(maxlen=2000)\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "      \n",
    "    def train_model(self, batch_size=20):\n",
    "        def batch(iterable, n=1):\n",
    "            l = len(iterable)\n",
    "            for ndx in range(0, l, n):\n",
    "                yield list(itertools.islice(iterable, ndx, min(ndx + n, l)))\n",
    "                \n",
    "                \n",
    "        batch_data = list(batch(self.memory, batch_size))\n",
    "        selected_batch = random.sample(batch_data, 1)\n",
    "        \n",
    "        for state, action, reward, next_state, done in selected_batch[0]:\n",
    "            \n",
    "            target = np.zeros((1, brain.value_size))\n",
    "            advantages = np.zeros((1, brain._action_size))\n",
    "            \n",
    "            value = brain.sess.run(brain.critic.Q, feed_dict={brain.critic.x: [state]}) # self.critic.predict(state)[0]\n",
    "            next_value = brain.sess.run(brain.critic.Q, feed_dict={brain.critic.x: [next_state]}) # self.critic.predict(next_state)[0]\n",
    "            \n",
    "            if done:\n",
    "                advantages[0][action] = reward - value\n",
    "                target[0][0] = reward\n",
    "            else:\n",
    "                advantages[0][action] = reward + brain.gamma * (next_value) - value\n",
    "                target[0][0] = reward + brain.gamma * next_value\n",
    "            \n",
    "            feed_dict_actor = {\n",
    "                brain.actor.x: [state],\n",
    "                brain.actor.advantages: advantages,\n",
    "            }\n",
    "            feed_dict_critic = {\n",
    "                brain.critic.x: [state], \n",
    "                brain.critic.targetQ: target,\n",
    "            }\n",
    "            _, actor_loss = brain.sess.run([brain.actor.train_op, brain.actor.loss], feed_dict=feed_dict_actor)\n",
    "#             summary = brain.sess.run([brain.actor.summary], feed_dict=feed_dict_actor)\n",
    "            _, critic_loss = brain.sess.run([brain.critic.train_op, brain.critic.loss], feed_dict=feed_dict_critic)\n",
    "#             summary = brain.sess.run([brain.critic.summary], feed_dict=feed_dict_critic)\n",
    "\n",
    "\n",
    "    # using the output of policy network, pick action stochastically\n",
    "    def takeAction(self, state):\n",
    "        return brain.sess.run(brain.actor._sample, feed_dict={brain.actor.x: [state]})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(threading.Thread):\n",
    "    stop_signal = False\n",
    "    ENV = 'CartPole-v0'\n",
    "    THREAD_DELAY = 0.001\n",
    "    \n",
    "    def __init__(self, render=False):\n",
    "        threading.Thread.__init__(self)\n",
    "\n",
    "        self.render = render\n",
    "        self.env = gym.make(self.ENV)\n",
    "        self.agent = A2CAgent()\n",
    "        \n",
    "        self.NUM_STATE = self.env.observation_space.shape[0]\n",
    "        self.NUM_ACTIONS = self.env.action_space.n\n",
    "\n",
    "        self.episodes = 0\n",
    "        self.shows = 20\n",
    "\n",
    "    def runEpisode(self):\n",
    "\n",
    "        state = self.env.reset()\n",
    "        Rewards = 0\n",
    "\n",
    "        while True: \n",
    "            time.sleep(self.THREAD_DELAY) # yield\n",
    "\n",
    "            if self.render: self.env.render()\n",
    "\n",
    "            action = self.agent.takeAction(state)\n",
    "            next_state, reward, done, _ = self.env.step(action)\n",
    "\n",
    "            self.agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            Rewards += reward\n",
    "\n",
    "            if done or self.stop_signal: break\n",
    "\n",
    "        self.agent.train_model(32)\n",
    "        \n",
    "        if Rewards >= 200:\n",
    "            self.stop()\n",
    "            \n",
    "        print(\"Episode {}, Reward: {}\".format(self.episodes, Rewards))\n",
    "        self.episodes += 1\n",
    "        \n",
    "            \n",
    "    def run(self):\n",
    "        while not self.stop_signal:\n",
    "            self.runEpisode()\n",
    "\n",
    "    def stop(self):\n",
    "        self.stop_signal = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Episode 0, Reward: 17.0\n",
      "Episode 0, Reward: 21.0\n",
      "Episode 0, Reward: 24.0\n",
      "Episode 0, Reward: 44.0\n",
      "Episode 1, Reward: 20.0\n",
      "Episode 1, Reward: 29.0\n",
      "Episode 1, Reward: 22.0\n",
      "Episode 1, Reward: 18.0\n",
      "Episode 2, Reward: 12.0\n",
      "Episode 2, Reward: 19.0\n",
      "Episode 2, Reward: 13.0\n",
      "Episode 3, Reward: 19.0\n",
      "Episode 2, Reward: 23.0\n",
      "Episode 3, Reward: 12.0Episode 3, Reward: 12.0\n",
      "\n",
      "Episode 4, Reward: 10.0\n",
      "Episode 3, Reward: 10.0\n",
      "Episode 4, Reward: 13.0\n",
      "Episode 5, Reward: 14.0\n",
      "Episode 4, Reward: 15.0\n",
      "Episode 5, Reward: 17.0\n",
      "Episode 4, Reward: 8.0\n",
      "Episode 6, Reward: 15.0\n",
      "Episode 5, Reward: 12.0\n",
      "Episode 6, Reward: 11.0\n",
      "Episode 5, Reward: 14.0\n",
      "Episode 7, Reward: 11.0\n",
      "Episode 6, Reward: 9.0\n",
      "Episode 7, Reward: 15.0\n",
      "Episode 6, Reward: 14.0\n",
      "Episode 8, Reward: 11.0\n",
      "Episode 7, Reward: 15.0\n",
      "Episode 7, Reward: 11.0\n",
      "Episode 8, Reward: 12.0\n",
      "Episode 8, Reward: 15.0\n",
      "Episode 9, Reward: 13.0\n",
      "Episode 8, Reward: 12.0\n",
      "Episode 9, Reward: 9.0\n",
      "Episode 9, Reward: 13.0\n",
      "Episode 9, Reward: 10.0\n",
      "Episode 10, Reward: 16.0\n",
      "Episode 10, Reward: 12.0\n",
      "Episode 10, Reward: 11.0\n",
      "Episode 10, Reward: 11.0\n",
      "Episode 11, Reward: 11.0\n",
      "Episode 11, Reward: 11.0\n",
      "Episode 11, Reward: 10.0\n",
      "Episode 11, Reward: 14.0\n",
      "Episode 12, Reward: 15.0\n",
      "Episode 12, Reward: 11.0\n",
      "Episode 12, Reward: 17.0\n",
      "Episode 12, Reward: 13.0\n",
      "Episode 13, Reward: 20.0\n",
      "Episode 13, Reward: 12.0Episode 13, Reward: 10.0\n",
      "\n",
      "Episode 13, Reward: 18.0\n",
      "Episode 14, Reward: 10.0\n",
      "Episode 14, Reward: 14.0\n",
      "Episode 14, Reward: 15.0\n",
      "Episode 15, Reward: 9.0\n",
      "Episode 14, Reward: 19.0\n",
      "Episode 15, Reward: 14.0\n",
      "Episode 15, Reward: 9.0\n",
      "Episode 16, Reward: 11.0\n",
      "Episode 16, Reward: 13.0\n",
      "Episode 15, Reward: 13.0\n",
      "Episode 17, Reward: 11.0\n",
      "Episode 16, Reward: 9.0\n",
      "Episode 17, Reward: 16.0\n",
      "Episode 16, Reward: 14.0\n",
      "Episode 18, Reward: 13.0\n",
      "Episode 18, Reward: 9.0\n",
      "Episode 17, Reward: 11.0\n",
      "Episode 17, Reward: 13.0\n",
      "Episode 19, Reward: 21.0\n",
      "Episode 18, Reward: 9.0\n",
      "Episode 19, Reward: 9.0\n",
      "Episode 18, Reward: 11.0\n",
      "Episode 19, Reward: 9.0\n",
      "Episode 20, Reward: 10.0\n",
      "Episode 20, Reward: 19.0\n",
      "Episode 19, Reward: 15.0\n",
      "Episode 21, Reward: 21.0\n",
      "Episode 20, Reward: 16.0\n",
      "Episode 20, Reward: 13.0\n",
      "Episode 21, Reward: 16.0\n",
      "Episode 21, Reward: 15.0\n",
      "Episode 22, Reward: 14.0\n",
      "Episode 21, Reward: 14.0\n",
      "Episode 22, Reward: 14.0\n",
      "Episode 22, Reward: 10.0\n",
      "Episode 23, Reward: 16.0\n",
      "Episode 22, Reward: 16.0\n",
      "Episode 23, Reward: 15.0\n",
      "Episode 23, Reward: 11.0\n",
      "Episode 24, Reward: 15.0\n",
      "Episode 23, Reward: 15.0\n",
      "Episode 24, Reward: 12.0\n",
      "Episode 24, Reward: 23.0\n",
      "Episode 25, Reward: 15.0\n",
      "Episode 24, Reward: 11.0\n",
      "Episode 25, Reward: 27.0\n",
      "Episode 25, Reward: 16.0\n",
      "Episode 26, Reward: 12.0\n",
      "Episode 25, Reward: 13.0\n",
      "Episode 26, Reward: 13.0\n",
      "Episode 26, Reward: 11.0\n",
      "Episode 27, Reward: 12.0\n",
      "Episode 26, Reward: 19.0\n",
      "Episode 27, Reward: 9.0\n",
      "Episode 27, Reward: 11.0\n",
      "Episode 28, Reward: 14.0\n",
      "Episode 28, Reward: 19.0\n",
      "Episode 27, Reward: 24.0\n",
      "Episode 29, Reward: 19.0\n",
      "Episode 28, Reward: 19.0\n",
      "Episode 29, Reward: 11.0\n",
      "Episode 28, Reward: 14.0\n",
      "Episode 30, Reward: 13.0\n",
      "Episode 29, Reward: 13.0\n",
      "Episode 30, Reward: 14.0\n",
      "Episode 29, Reward: 25.0\n",
      "Episode 30, Reward: 10.0\n",
      "Episode 31, Reward: 45.0\n",
      "Episode 31, Reward: 14.0\n",
      "Episode 31, Reward: 15.0\n",
      "Episode 30, Reward: 17.0\n",
      "Episode 32, Reward: 25.0\n",
      "Episode 32, Reward: 41.0\n",
      "Episode 32, Reward: 12.0\n",
      "Episode 31, Reward: 55.0\n",
      "Episode 33, Reward: 33.0\n",
      "Episode 33, Reward: 50.0\n",
      "Episode 32, Reward: 36.0\n",
      "Episode 33, Reward: 79.0\n",
      "Episode 34, Reward: 31.0\n",
      "Episode 34, Reward: 17.0\n",
      "Episode 34, Reward: 27.0\n",
      "Episode 35, Reward: 28.0\n",
      "Episode 35, Reward: 46.0\n",
      "Episode 35, Reward: 34.0\n",
      "Episode 33, Reward: 143.0\n",
      "Episode 36, Reward: 35.0\n",
      "Episode 36, Reward: 96.0\n",
      "Episode 37, Reward: 42.0\n",
      "Episode 36, Reward: 30.0\n",
      "Episode 34, Reward: 67.0\n",
      "Episode 37, Reward: 69.0\n",
      "Episode 38, Reward: 30.0\n",
      "Episode 35, Reward: 18.0\n",
      "Episode 37, Reward: 97.0\n",
      "Episode 36, Reward: 14.0\n",
      "Episode 39, Reward: 38.0\n",
      "Episode 38, Reward: 68.0\n",
      "Episode 38, Reward: 12.0\n",
      "Episode 39, Reward: 28.0\n",
      "Episode 39, Reward: 29.0\n",
      "Episode 40, Reward: 46.0\n",
      "Episode 37, Reward: 60.0\n",
      "Episode 40, Reward: 34.0\n",
      "Episode 40, Reward: 16.0\n",
      "Episode 38, Reward: 42.0\n",
      "Episode 41, Reward: 65.0\n",
      "Episode 41, Reward: 33.0\n",
      "Episode 39, Reward: 29.0\n",
      "Episode 42, Reward: 29.0\n",
      "Episode 41, Reward: 131.0\n",
      "Episode 40, Reward: 21.0\n",
      "Episode 42, Reward: 75.0\n",
      "Episode 43, Reward: 51.0\n",
      "Episode 42, Reward: 23.0\n",
      "Episode 43, Reward: 34.0\n",
      "Episode 41, Reward: 35.0\n",
      "Episode 43, Reward: 21.0\n",
      "Episode 44, Reward: 59.0\n",
      "Episode 44, Reward: 30.0\n",
      "Episode 42, Reward: 53.0\n",
      "Episode 44, Reward: 36.0\n",
      "Episode 45, Reward: 35.0\n",
      "Episode 45, Reward: 62.0\n",
      "Episode 46, Reward: 18.0\n",
      "Episode 43, Reward: 40.0\n",
      "Episode 45, Reward: 31.0\n",
      "Episode 46, Reward: 74.0\n",
      "Episode 47, Reward: 13.0\n",
      "Episode 47, Reward: 62.0\n",
      "Episode 44, Reward: 85.0\n",
      "Episode 46, Reward: 81.0\n",
      "Episode 48, Reward: 52.0\n",
      "Episode 48, Reward: 46.0\n",
      "Episode 45, Reward: 41.0\n",
      "Episode 49, Reward: 25.0\n",
      "Episode 49, Reward: 37.0\n",
      "Episode 47, Reward: 47.0\n",
      "Episode 50, Reward: 19.0\n",
      "Episode 48, Reward: 44.0\n",
      "Episode 50, Reward: 87.0\n",
      "Episode 46, Reward: 105.0\n",
      "Episode 49, Reward: 40.0\n",
      "Episode 51, Reward: 54.0\n",
      "Episode 51, Reward: 43.0\n",
      "Episode 47, Reward: 52.0\n",
      "Episode 52, Reward: 30.0\n",
      "Episode 50, Reward: 73.0\n",
      "Episode 48, Reward: 40.0\n",
      "Episode 52, Reward: 72.0\n",
      "Episode 53, Reward: 55.0\n",
      "Episode 51, Reward: 87.0\n",
      "Episode 53, Reward: 39.0\n",
      "Episode 49, Reward: 97.0\n",
      "Episode 54, Reward: 38.0\n",
      "Episode 52, Reward: 48.0\n",
      "Episode 54, Reward: 105.0\n",
      "Episode 50, Reward: 65.0\n",
      "Episode 53, Reward: 95.0\n",
      "Episode 55, Reward: 81.0\n",
      "Episode 55, Reward: 117.0\n",
      "Episode 51, Reward: 156.0\n",
      "Episode 54, Reward: 61.0\n",
      "Episode 56, Reward: 70.0\n",
      "Episode 56, Reward: 77.0\n",
      "Episode 55, Reward: 18.0\n",
      "Episode 52, Reward: 74.0\n",
      "Episode 57, Reward: 48.0\n",
      "Episode 57, Reward: 76.0\n",
      "Episode 58, Reward: 31.0\n",
      "Episode 58, Reward: 57.0\n",
      "Episode 53, Reward: 121.0\n",
      "Episode 56, Reward: 200.0\n",
      "Episode 54, Reward: 30.0\n",
      "Episode 59, Reward: 47.0\n",
      "Episode 59, Reward: 162.0\n",
      "Episode 60, Reward: 72.0\n",
      "Episode 55, Reward: 106.0\n",
      "Episode 60, Reward: 79.0\n",
      "Episode 61, Reward: 29.0\n",
      "Episode 56, Reward: 82.0\n",
      "Episode 61, Reward: 55.0\n",
      "Episode 62, Reward: 57.0\n",
      "Episode 62, Reward: 45.0\n",
      "Episode 57, Reward: 29.0\n",
      "Episode 58, Reward: 14.0\n",
      "Episode 63, Reward: 115.0\n",
      "Episode 63, Reward: 85.0\n",
      "Episode 64, Reward: 95.0\n",
      "Episode 64, Reward: 108.0\n",
      "Episode 59, Reward: 180.0\n",
      "Episode 60, Reward: 38.0\n",
      "Episode 65, Reward: 126.0\n",
      "Episode 65, Reward: 29.0\n",
      "Episode 61, Reward: 11.0\n",
      "Episode 66, Reward: 56.0\n",
      "Episode 62, Reward: 47.0\n",
      "Episode 66, Reward: 89.0\n",
      "Episode 63, Reward: 37.0\n",
      "Episode 67, Reward: 21.0\n",
      "Episode 67, Reward: 87.0\n",
      "Episode 64, Reward: 23.0\n",
      "Episode 68, Reward: 26.0\n",
      "Episode 65, Reward: 41.0\n",
      "Episode 68, Reward: 135.0\n",
      "Episode 66, Reward: 49.0\n",
      "Episode 69, Reward: 123.0\n",
      "Episode 69, Reward: 63.0\n",
      "Episode 67, Reward: 45.0\n",
      "Episode 70, Reward: 29.0\n",
      "Episode 70, Reward: 49.0\n",
      "Episode 68, Reward: 114.0\n",
      "Episode 71, Reward: 167.0\n",
      "Episode 71, Reward: 176.0\n",
      "Episode 69, Reward: 68.0\n",
      "Episode 72, Reward: 36.0\n",
      "Episode 72, Reward: 71.0\n",
      "Episode 73, Reward: 47.0\n",
      "Episode 70, Reward: 104.0\n",
      "Episode 73, Reward: 125.0\n",
      "Episode 74, Reward: 83.0\n",
      "Episode 71, Reward: 136.0\n",
      "Episode 74, Reward: 54.0\n",
      "Episode 75, Reward: 101.0\n",
      "Episode 75, Reward: 31.0\n",
      "Episode 72, Reward: 129.0\n",
      "Episode 76, Reward: 88.0\n",
      "Episode 76, Reward: 89.0\n",
      "Episode 73, Reward: 116.0\n",
      "Episode 77, Reward: 67.0\n",
      "Episode 77, Reward: 51.0\n",
      "Episode 78, Reward: 92.0\n",
      "Episode 74, Reward: 177.0\n",
      "Episode 78, Reward: 134.0\n",
      "Episode 75, Reward: 31.0\n",
      "Episode 79, Reward: 138.0\n",
      "Episode 79, Reward: 51.0\n",
      "Episode 76, Reward: 109.0\n",
      "Episode 77, Reward: 44.0\n",
      "Episode 80, Reward: 114.0\n",
      "Episode 80, Reward: 131.0\n",
      "Episode 81, Reward: 55.0\n",
      "Episode 81, Reward: 57.0\n",
      "Episode 82, Reward: 23.0\n",
      "Episode 78, Reward: 139.0\n",
      "Training finished\n",
      "Episode 0, Reward: 32.0\n",
      "Episode 1, Reward: 169.0\n",
      "Episode 2, Reward: 77.0\n",
      "Episode 3, Reward: 38.0\n",
      "Episode 4, Reward: 50.0\n",
      "Episode 5, Reward: 125.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 6, Reward: 51.0\n",
      "Episode 7, Reward: 95.0\n",
      "Episode 8, Reward: 98.0\n",
      "Episode 9, Reward: 44.0\n",
      "Episode 10, Reward: 200.0\n"
     ]
    }
   ],
   "source": [
    "env_test = Environment(render=True)\n",
    "RUN_TIME = 30\n",
    "THREADS = 4\n",
    "\n",
    "brain = Brain(env_test.NUM_STATE, env_test.NUM_ACTIONS)    # brain is global in A3C\n",
    "\n",
    "envs = [Environment() for i in range(THREADS)]\n",
    "\n",
    "for e in envs:\n",
    "    e.start()\n",
    "\n",
    "time.sleep(RUN_TIME)\n",
    "\n",
    "for e in envs:\n",
    "    e.stop()\n",
    "for e in envs:\n",
    "    e.join()\n",
    "\n",
    "print(\"Training finished\")\n",
    "env_test.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py36(openAI gym)",
   "language": "python",
   "name": "openai-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
