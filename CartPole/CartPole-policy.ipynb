{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CartPole-v1\n",
    "\n",
    "> reference: https://github.com/rlcode/reinforcement-learning/blob/master/2-cartpole/3-reinforce/cartpole_reinforce.py\n",
    "\n",
    "A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. \n",
    "\n",
    "A `reward` of +1 is provided for every timestep that the pole remains upright. The episode ends when `the pole is more than 15 degrees from vertical`, or `the cart moves more than 2.4 units from the center`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.callbacks import History\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class policyModel():\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self._state_size = state_size\n",
    "        self._action_size = action_size\n",
    "        self._learning_rate = 0.001\n",
    "        self.discount_factor = 0.99\n",
    "        self.model = self._build_model()\n",
    "        self.memory = list()\n",
    "    \n",
    "    def _build_model(self, hidden_size=24):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(hidden_size, activation='relu', kernel_initializer='glorot_uniform', input_shape=(self._state_size,)))\n",
    "        model.add(Dense(hidden_size, activation='relu', kernel_initializer='glorot_uniform', input_dim=hidden_size))\n",
    "        model.add(Dense(self._action_size, activation='softmax', kernel_initializer='glorot_uniform', input_dim=hidden_size))\n",
    "        opt = Adam(lr=self._learning_rate)\n",
    "        \n",
    "        # Using categorical crossentropy as a loss is a trick to easily\n",
    "        # implement the policy gradient. Categorical cross entropy is defined\n",
    "        # H(p, q) = sum(p_i * log(q_i)). For the action taken, a, you set \n",
    "        # p_a = advantage. q_a is the output of the policy network, which is\n",
    "        # the probability of taking the action a, i.e. policy(s, a). \n",
    "        # All other p_i are zero, thus we have H(p, q) = A * log(policy(s, a))\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=opt)\n",
    "        model.summary()\n",
    "        return model\n",
    "    \n",
    "    def remember(self, state, action, reward):\n",
    "        self.memory.append((state, action, reward))\n",
    "    \n",
    "    def discount_rewards(self, rewards):\n",
    "        discounted_rewards = np.zeros_like(rewards)\n",
    "        running_add = 0\n",
    "        for t in reversed(range(0, len(rewards))):\n",
    "            running_add = running_add * self.discount_factor + rewards[t]\n",
    "            discounted_rewards[t] = running_add\n",
    "        return discounted_rewards\n",
    "    \n",
    "    def train_model(self):\n",
    "        history = History()\n",
    "        states, actions, rewards = list(zip(*self.memory))\n",
    "        states = np.array(states)\n",
    "        episode_length = len(agent.memory)\n",
    "\n",
    "        discounted_rewards = self.discount_rewards(rewards)\n",
    "        baseline = np.mean(discounted_rewards)\n",
    "        discounted_rewards -= baseline\n",
    "        discounted_rewards /= np.std(discounted_rewards)\n",
    "\n",
    "        advantages = np.zeros((episode_length, self._action_size))\n",
    "        for i in range(episode_length):\n",
    "            advantages[i][actions[i]] = discounted_rewards[i]\n",
    "\n",
    "        self.model.fit(states, advantages, epochs=1, verbose=0, callbacks=[history])\n",
    "#         print(history.history)\n",
    "        self.memory = list()\n",
    "\n",
    "    def takeAction(self, state):\n",
    "        state = state.reshape(1,self._state_size)\n",
    "\n",
    "        # using the output of policy network, pick action stochastically\n",
    "        policy = self.model.predict(state).flatten()\n",
    "        return np.random.choice(self._action_size, 1, p=policy)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 24)                120       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 50        \n",
      "=================================================================\n",
      "Total params: 770\n",
      "Trainable params: 770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Episode: 0, Total Reward: 15.0\n",
      "Episode: 20, Total Reward: 16.7\n",
      "Episode: 40, Total Reward: 21.0\n",
      "Episode: 60, Total Reward: 18.9\n",
      "Episode: 80, Total Reward: 21.45\n",
      "Episode: 100, Total Reward: 17.95\n",
      "Episode: 120, Total Reward: 25.3\n",
      "Episode: 140, Total Reward: 24.55\n",
      "Episode: 160, Total Reward: 26.8\n",
      "Episode: 180, Total Reward: 24.65\n",
      "Episode: 200, Total Reward: 31.3\n",
      "Episode: 220, Total Reward: 24.65\n",
      "Episode: 240, Total Reward: 26.1\n",
      "Episode: 260, Total Reward: 28.05\n",
      "Episode: 280, Total Reward: 31.65\n",
      "Episode: 300, Total Reward: 32.55\n",
      "Episode: 320, Total Reward: 30.25\n",
      "Episode: 340, Total Reward: 32.0\n",
      "Episode: 360, Total Reward: 40.0\n",
      "Episode: 380, Total Reward: 38.2\n",
      "Episode: 400, Total Reward: 49.15\n",
      "Episode: 420, Total Reward: 44.85\n",
      "Episode: 440, Total Reward: 38.85\n",
      "Episode: 460, Total Reward: 44.1\n",
      "Episode: 480, Total Reward: 56.5\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "state = env.reset()\n",
    "agent = policyModel(state.shape[0], env.action_space.n)\n",
    "\n",
    "total_reward = list()\n",
    "render = False\n",
    "shows = 20\n",
    "\n",
    "for i_episode in range(500):\n",
    "    \n",
    "    state = env.reset()\n",
    "    score = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        if render:\n",
    "            env.render()  \n",
    "    \n",
    "        action = agent.takeAction(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        agent.remember(state, action, reward)\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        \n",
    "        if done:\n",
    "            agent.train_model()\n",
    "            total_reward.append(score)\n",
    "    \n",
    "    if i_episode % shows == 0:\n",
    "        print(\"Episode: {}, Total Reward: {}\".format(i_episode, np.mean(total_reward[-shows:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py36(openAI gym)",
   "language": "python",
   "name": "openai-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
