{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CartPole-v0\n",
    "\n",
    "> reference: \n",
    "\n",
    "A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. \n",
    "\n",
    "A `reward` of +1 is provided for every timestep that the pole remains upright. The episode ends when `the pole is more than 15 degrees from vertical`, or `the cart moves more than 2.4 units from the center`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.callbacks import History\n",
    "from collections import deque\n",
    "import itertools\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2CAgent():\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self._state_size = state_size\n",
    "        self._action_size = action_size        \n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self._learning_rate = 0.001\n",
    "        \n",
    "        self.value_size = 1\n",
    "        self.actor, self.critic = self._build_model()\n",
    "        \n",
    "    def _build_model(self, hidden_size=24):\n",
    "        state = Input(batch_shape=(None,  self._state_size))\n",
    "        shared = Dense(hidden_size, input_dim=self._state_size, activation='relu', kernel_initializer='glorot_uniform')(state)\n",
    "\n",
    "        actor_hidden = Dense(hidden_size, activation='relu', kernel_initializer='glorot_uniform')(shared)\n",
    "        action_prob = Dense(self._action_size, activation='softmax', kernel_initializer='glorot_uniform')(actor_hidden)\n",
    "        actor = Model(inputs=state, outputs=action_prob)\n",
    "        \n",
    "        value_hidden = Dense(hidden_size, activation='relu', kernel_initializer='he_uniform')(shared)\n",
    "        state_value = Dense(1, activation='linear', kernel_initializer='he_uniform')(value_hidden)\n",
    "        critic = Model(inputs=state, outputs=state_value)\n",
    "\n",
    "        opt = Adam(lr=self._learning_rate)\n",
    "        actor.compile(loss='categorical_crossentropy', optimizer=opt)\n",
    "        critic.compile(loss='mean_squared_error', optimizer=opt)\n",
    "        \n",
    "        actor.summary()\n",
    "        critic.summary()\n",
    "\n",
    "        return actor, critic\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "      \n",
    "    # update policy network every episode\n",
    "    def train_model(self, batch_size=20):\n",
    "        def batch(iterable, n=1):\n",
    "            l = len(iterable)\n",
    "            for ndx in range(0, l, n):\n",
    "                yield list(itertools.islice(iterable, ndx, min(ndx + n, l)))\n",
    "        \n",
    "        history = History()\n",
    "        batch_data = list(batch(self.memory, batch_size))\n",
    "        selected_batch = random.sample(batch_data, 1)\n",
    "        \n",
    "        for state, action, reward, next_state, done in selected_batch[0]:\n",
    "            target = np.zeros((1, self.value_size))\n",
    "            advantages = np.zeros((1, self._action_size))\n",
    "\n",
    "            value = self.critic.predict(state)[0]\n",
    "            next_value = self.critic.predict(next_state)[0]\n",
    "\n",
    "            if done:\n",
    "                advantages[0][action] = reward - value\n",
    "                target[0][0] = reward\n",
    "            else:\n",
    "                advantages[0][action] = reward + self.gamma * (next_value) - value\n",
    "                target[0][0] = reward + self.gamma * next_value\n",
    "\n",
    "            self.actor.fit(state, advantages, epochs=1, verbose=0)\n",
    "            self.critic.fit(state, target, epochs=1, verbose=0) \n",
    "\n",
    "    # using the output of policy network, pick action stochastically\n",
    "    def takeAction(self, state):\n",
    "        policy = self.actor.predict(state, batch_size=1).flatten()\n",
    "        return np.random.choice(self._action_size, 1, p=policy)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 24)                120       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 50        \n",
      "=================================================================\n",
      "Total params: 770\n",
      "Trainable params: 770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 24)                120       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 25        \n",
      "=================================================================\n",
      "Total params: 745\n",
      "Trainable params: 745\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Episode: 0, Total Reward: 9.0\n",
      "Episode: 20, Total Reward: 10.55\n",
      "Episode: 40, Total Reward: 8.55\n",
      "Episode: 60, Total Reward: 11.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuting/yuting_data/github/openAI/.venv/lib/python3.6/site-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.119619). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 80, Total Reward: 10.95\n",
      "Episode: 100, Total Reward: 10.1\n",
      "Episode: 120, Total Reward: 33.75\n",
      "Episode: 140, Total Reward: 37.25\n",
      "Episode: 160, Total Reward: 70.45\n",
      "Episode: 180, Total Reward: 81.1\n",
      "Episode: 200, Total Reward: 39.75\n",
      "Episode: 220, Total Reward: 69.75\n",
      "Episode: 240, Total Reward: 95.5\n",
      "Episode: 260, Total Reward: 45.35\n",
      "Episode: 280, Total Reward: 74.0\n",
      "Episode: 300, Total Reward: 101.2\n",
      "Episode: 320, Total Reward: 133.0\n",
      "Episode: 340, Total Reward: 156.15\n",
      "Episode: 360, Total Reward: 167.4\n",
      "Episode: 380, Total Reward: 100.6\n",
      "Episode: 400, Total Reward: 134.4\n",
      "Episode: 420, Total Reward: 30.45\n",
      "Episode: 440, Total Reward: 150.55\n",
      "Episode: 460, Total Reward: 172.15\n",
      "Episode: 480, Total Reward: 169.45\n"
     ]
    }
   ],
   "source": [
    "episodes = 500\n",
    "render = False\n",
    "shows = 20\n",
    "total_reward = list()\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "state = env.reset()\n",
    "agent = A2CAgent(state.shape[0], env.action_space.n)\n",
    "\n",
    "for e in range(episodes):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, 4])\n",
    "    \n",
    "    for time_t in range(500):\n",
    "        \n",
    "        if render:\n",
    "            env.render()\n",
    "            \n",
    "        action = agent.takeAction(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        next_state = np.reshape(next_state, [1, 4])\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            total_reward.append(time_t)\n",
    "            break\n",
    "            \n",
    "    agent.train_model(32)\n",
    "    \n",
    "    if e % shows == 0:\n",
    "        print(\"Episode: {}, Total Reward: {}\".format(e, np.mean(total_reward[-shows:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py36(openAI gym)",
   "language": "python",
   "name": "openai-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
