{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpaceInvaders-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, merge, Permute, Conv2D\n",
    "from keras.models import load_model, Sequential, Model\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.core import Activation, Dropout, Flatten, Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.callbacks import History\n",
    "import keras\n",
    "import os\n",
    "\n",
    "from collections import deque\n",
    "import itertools\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent():\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self._state_size = state_size\n",
    "        self._action_size = action_size        \n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self._learning_rate = 0.001\n",
    "        self.model_name = 'SpaceInvaders-DQN.h5'\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "    \n",
    "    \n",
    "    def _build_model(self, hidden_size=24):\n",
    "            \n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, (8, 8), strides=(4, 4), input_shape=self._state_size))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Conv2D(64, (4, 4), strides=(2, 2)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Conv2D(64, (3, 3), strides=(1, 1)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(self._action_size))\n",
    "        model.add(Activation('linear'))\n",
    "        print(model.summary())\n",
    "\n",
    "        opt = Adam(lr=self._learning_rate)\n",
    "        model.compile(loss='mean_squared_error', optimizer=opt)\n",
    "        model.summary()\n",
    "        \n",
    "        if os.path.exists(self.model_name):\n",
    "            model.load_weights(self.model_name)\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        state = state.reshape(1,210, 160, 3)\n",
    "        next_state = next_state.reshape(1,210, 160, 3)\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def update_target_model(self):\n",
    "        self.target_model = self.model\n",
    "        \n",
    "    def train_model(self, batch_size=20):\n",
    "        def batch(iterable, n=1):\n",
    "            l = len(iterable)\n",
    "            for ndx in range(0, l, n):\n",
    "                yield list(itertools.islice(iterable, ndx, min(ndx + n, l)))\n",
    "        \n",
    "        history = History()\n",
    "        batch_data = list(batch(self.memory, batch_size))\n",
    "        selected_batch = random.sample(batch_data, 1)\n",
    "        \n",
    "        for state, action, reward, next_state, done in selected_batch[0]:\n",
    "            target = self.model.predict(state)\n",
    "            target_val = self.model.predict(next_state)\n",
    "            target_val_ = self.target_model.predict(next_state)\n",
    "            if done:\n",
    "                target[0][action] = reward\n",
    "            else:\n",
    "                a = np.argmax(target_val)\n",
    "                target[0][action] = reward + self.gamma * target_val_[0][a]\n",
    "            \n",
    "            self.model.fit(state, target, epochs=1, verbose=0, callbacks=[history])\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay    \n",
    "\n",
    "    def takeAction(self, state):\n",
    "        state = state.reshape(1,210, 160, 3)\n",
    "        act = np.argmax(self.model.predict(state))\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            act = random.randrange(self._action_size)\n",
    "        return act\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Total Reward: 115.0\n",
      "Episode: 10, Total Reward: 140.5\n",
      "Episode: 20, Total Reward: 198.5\n",
      "Episode: 30, Total Reward: 260.0\n",
      "Episode: 40, Total Reward: 203.5\n",
      "Episode: 50, Total Reward: 245.0\n",
      "Episode: 60, Total Reward: 159.0\n",
      "Episode: 70, Total Reward: 231.0\n",
      "Episode: 80, Total Reward: 147.5\n",
      "Episode: 90, Total Reward: 253.0\n",
      "Episode: 100, Total Reward: 214.5\n",
      "Episode: 110, Total Reward: 288.5\n",
      "Episode: 120, Total Reward: 226.0\n",
      "Episode: 130, Total Reward: 136.5\n",
      "Episode: 140, Total Reward: 202.5\n",
      "Episode: 150, Total Reward: 201.0\n",
      "Episode: 160, Total Reward: 205.0\n",
      "Episode: 170, Total Reward: 160.0\n",
      "Episode: 180, Total Reward: 192.0\n",
      "Episode: 190, Total Reward: 214.5\n",
      "Episode: 200, Total Reward: 175.0\n",
      "Episode: 210, Total Reward: 122.0\n",
      "Episode: 220, Total Reward: 223.0\n",
      "Episode: 230, Total Reward: 216.0\n",
      "Episode: 240, Total Reward: 153.0\n",
      "Episode: 250, Total Reward: 196.0\n",
      "Episode: 260, Total Reward: 178.5\n",
      "Episode: 270, Total Reward: 178.0\n",
      "Episode: 280, Total Reward: 176.5\n",
      "Episode: 290, Total Reward: 131.5\n",
      "Episode: 300, Total Reward: 194.0\n",
      "Episode: 310, Total Reward: 142.0\n",
      "Episode: 320, Total Reward: 197.0\n",
      "Episode: 330, Total Reward: 126.0\n",
      "Episode: 340, Total Reward: 140.0\n",
      "Episode: 350, Total Reward: 150.0\n",
      "Episode: 360, Total Reward: 127.5\n",
      "Episode: 370, Total Reward: 162.5\n",
      "Episode: 380, Total Reward: 112.0\n",
      "Episode: 390, Total Reward: 222.5\n",
      "Episode: 400, Total Reward: 135.0\n",
      "Episode: 410, Total Reward: 162.0\n",
      "Episode: 420, Total Reward: 133.5\n",
      "Episode: 430, Total Reward: 93.5\n",
      "Episode: 440, Total Reward: 177.5\n",
      "Episode: 450, Total Reward: 142.5\n",
      "Episode: 460, Total Reward: 79.0\n",
      "Episode: 470, Total Reward: 189.0\n",
      "Episode: 480, Total Reward: 121.0\n",
      "Episode: 490, Total Reward: 205.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 500\n",
    "render = True\n",
    "shows = 10\n",
    "total_reward = list()\n",
    "\n",
    "env = gym.make('SpaceInvaders-v0')\n",
    "agent = DQNAgent(env.observation_space.shape, env.action_space.n)\n",
    "\n",
    "for e in range(episodes):\n",
    "    state = env.reset()\n",
    "    \n",
    "    rewards = 0\n",
    "    while True:\n",
    "        \n",
    "        if render:\n",
    "            env.render()\n",
    "        action = agent.takeAction(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        rewards += reward\n",
    "        \n",
    "        if done:\n",
    "            total_reward.append(rewards)\n",
    "            agent.remember(state, action, rewards, next_state, done)\n",
    "            state = next_state\n",
    "#             print(\"Episode: {}, Total Reward: {}\".format(e, rewards))\n",
    "            break\n",
    "        else:\n",
    "            agent.remember(state, action, 0, next_state, done)\n",
    "            state = next_state\n",
    "            \n",
    "    agent.train_model(32)\n",
    "    \n",
    "    if e % shows == 0:\n",
    "        print(\"Episode: {}, Total Reward: {}\".format(e, np.mean(total_reward[-shows:])))\n",
    "        agent.update_target_model()\n",
    "        agent.model.save_weights(agent.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py36(openAI gym)",
   "language": "python",
   "name": "openai-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
